# Baseline Solvers Analysis

Based on the mathematical formulation in `feedback2.tex` and the code in `baseline_solvers.py`, here is a step-by-step walkthrough of the baseline solvers.

## 1. How it Relates to Your Problem
According to `feedback2.tex`, your project (Project 34) focuses on solving a **Convex Quadratic Program (QP)** where the feasible region is a Cartesian product of standard simplices. 
Mathematically, you are minimizing:
$$ \min \frac{1}{2} x^T Q x + q^T x $$
Subject to:
$$ \sum_{i \in I^k} x_i = 1 \quad \forall k \in K $$
$$ x \ge 0 $$
Where $I^k$ are disjoint blocks of indices. This means your variables are grouped, and each group must sum to 1 and be non-negative (like a probability distribution).

Your project aims to build a highly specialized **Primal-Dual Interior-Point Method (IPM)** that exploits this exact block structure to solve the KKT system efficiently. The `baseline_solvers.py` file provides standard, off-the-shelf implementations of this exact problem. They serve two purposes:
1. **Ground Truth:** To verify that your custom IPM returns the correct mathematical solution.
2. **Performance Benchmark:** To prove that your specialized IPM is faster than general-purpose solvers.

Here is the step-by-step breakdown of the decisions and optimizations made in both baseline implementations.

---

## 2. Walkthrough: `solve_baseline_cvxpy`

CVXPY is a modeling language that translates mathematical problems into standard forms and passes them to dedicated C-level solvers.

**Step 1: Input Sanitization & Symmetrization**

```python
Q = np.asarray(Q, dtype=float)
if scipy.sparse.issparse(Q):
    Q = Q.toarray()
if not np.allclose(Q, Q.T):
    Q = (Q + Q.T) / 2
```
*   **Decision:** Convert sparse matrices to dense arrays and force exact symmetry.
*   **Why:** Many QP solvers will crash or throw a "not positive semi-definite" error if $Q$ has even a $10^{-16}$ floating-point asymmetry. Averaging $Q$ with its transpose guarantees perfect mathematical symmetry. Converting to dense ensures maximum compatibility with CVXPY's parser, prioritizing robustness over memory efficiency for the baseline.

**Step 2: Variables and Bounds**
```python
x = cp.Variable(n, nonneg=True)
```
*   **Decision:** Use the `nonneg=True` flag.
*   **Why:** This directly encodes the $x \ge 0$ constraint. CVXPY handles variable bounds much more efficiently than if you were to write an explicit constraint like `constraints.append(x >= 0)`.

**Step 3: Simplex Constraints**
```python
constraints = []
for k, block in enumerate(blocks):
    constraints.append(cp.sum(x[block]) == 1.0)
```
*   **Decision:** Loop through the index blocks and enforce the sum to 1.
*   **Why:** This perfectly maps to your $\sum_{i \in I^k} x_i = 1$ constraint.

**Step 4: Objective Function**
```python
objective = cp.Minimize(0.5 * cp.quad_form(x, Q) + q @ x)
```
*   **Decision:** Use `cp.quad_form(x, Q)` instead of `x.T @ Q @ x`.
*   **Why:** This is a crucial CVXPY optimization. `quad_form` explicitly tells CVXPY's Disciplined Convex Programming (DCP) analyzer that this is a quadratic form. If $Q$ is positive semi-definite, CVXPY instantly knows the problem is convex and can route it to fast QP solvers.

**Step 5: Solver Selection and Fallback**
```python
problem.solve(solver=cp.OSQP, verbose=False)
if problem.status not in ['optimal', 'optimal_inaccurate']:
    problem.solve(solver=cp.ECOS, verbose=False)
```
*   **Decision:** Try OSQP first, fallback to ECOS.
*   **Why:** OSQP (Operator Splitting QP) is an ADMM-based first-order solver. It is extremely fast for QPs but can sometimes struggle with high-accuracy tolerances. If OSQP fails to find an optimal solution, the code falls back to ECOS, which is a robust, second-order Interior-Point solver (very similar to the theory you are studying in `feedback2.tex`).

---

## 3. Walkthrough: `solve_baseline_scipy`
SciPy's `minimize` is a general-purpose nonlinear optimizer. It does not know your problem is a QP unless you guide it.

**Step 1: Analytical Gradient (Crucial Optimization)**
```python
def objective(x):
    return 0.5 * x.T @ Q @ x + q @ x

def gradient(x):
    return Q @ x + q
```
*   **Decision:** Explicitly define and provide the exact mathematical gradient (Jacobian) of the objective.
*   **Why:** If you don't provide the `jac` argument, SciPy will approximate the gradient using finite differences (evaluating the objective function $n$ extra times per step). For large $n$, this is catastrophically slow and numerically unstable. Providing $Qx + q$ makes SciPy exponentially faster.

**Step 2: Constraints and Bounds**
```python
# Constraints: Ex = 1 (equality constraints)
# OLD APPROACH (Nested closures - commented out for clarity)
# constraints = []
# for k, block in enumerate(blocks):
#     # Create constraint: sum(x[block]) == 1
#     def make_constraint(block_indices):
#         def constraint_eq(x):
#             return np.sum(x[block_indices]) - 1.0
#         return {'type': 'eq', 'fun': constraint_eq}
#     constraints.append(make_constraint(block))

# NEW APPROACH (Single vector function)
def eq_constraints(x):
    # Return an array where each element is (sum(x[block]) - 1.0)
    # SciPy will enforce that every element in this array equals 0.
    return np.array([np.sum(x[block]) - 1.0 for block in blocks])

constraints = [{'type': 'eq', 'fun': eq_constraints}]

# Bounds
bounds = [(0, None) for _ in range(n)]
```
*   **Decision (Old Approach - No longer used):** The old approach used a nested closure (`make_constraint`) inside a loop to generate a list of individual constraint functions. This is a classic Python trick to avoid "late binding" issues in loops, but it is overly complex and hard to read.
*   **Decision (New Approach):** Instead of creating $K$ different constraint functions, we create **one** function (`eq_constraints`) that returns an array of size $K$. SciPy understands that if `type` is `'eq'`, every element in the returned array must be zero. This is much cleaner, easier to explain, and represents a more standard way to use SciPy.
*   **Decision (Bounds):** Separate the $x \ge 0$ bounds from the equality constraints. This is the standard, idiomatic way to handle simple variable limits in SciPy.
*   **Why:** SciPy's SLSQP algorithm handles simple box bounds `(0, None)` internally via active-set methods, which is highly efficient. If you had passed $x \ge 0$ as a standard inequality constraint, the solver would have to compute a much larger constraint Jacobian, slowing it down significantly. It is standard practice in optimization libraries to treat simple variable bounds separately from general linear or nonlinear constraints because the underlying algorithms can exploit the trivial structure of bounds (they are just axis-aligned hyperplanes) to speed up the search.

**Step 3: The Initial Guess ($x_0$)**
```python
x0 = np.zeros(n)
for k, block in enumerate(blocks):
    x0[block] = 1.0 / len(block)
```
*   **Decision:** Start the solver at the exact center of the simplices (e.g., if a block has 4 variables, start them all at 0.25).
*   **Why:** This is a **strictly feasible starting point**. As mentioned in your `feedback2.tex` regarding "feasible-start" methods, starting inside the feasible region prevents the solver from wasting iterations trying to find a valid point. It also prevents the solver from getting stuck on the boundary ($x=0$) early in the optimization process.

**Step 4: Solver Choice**
```python
res = minimize(..., method='SLSQP', jac=gradient, bounds=bounds, constraints=constraints, ...)
```
*   **Decision:** Use `SLSQP` (Sequential Least SQuares Programming).
*   **Why:** Among SciPy's built-in solvers, SLSQP is the only robust one that simultaneously supports both equality constraints (the simplex sums) and bounds (non-negativity). 

### Summary
These baselines are written to be as robust and fast as possible using standard libraries. However, because they are general-purpose, they cannot exploit the specific block-diagonal structure of your constraints when solving the KKT linear systems. Your custom IPM (described in `feedback2.tex`) will theoretically outperform them by using the Schur-complement reduction specifically tailored to the $Ex = \mathbf{1}$ constraint matrix.