# Algorithm 1 Final Solver Analysis: `algo1_final_solver.py`

This document provides a deep dive into the core solver of the project: `FeasibleStartIPM`. This class implements the Primal-Dual Feasible-Start Interior-Point Method described in `feedback2.tex`. 

Unlike the naive baseline (`algo1_baseline_solver.py`), this implementation is designed for high performance, numerical stability, and scalability, heavily utilizing the matrix-free operators from `block_ops.py` and the numerical safeguards from `utils.py`.

---

## 1. Initialization and Configuration

### 1.1 `__init__` and `default_config`
```python
class FeasibleStartIPM:
    def __init__(self, Q, q, blocks, cfg=None):
        # Convert Q to numpy array or sparse matrix
        if scipy.sparse.issparse(Q):
            self.Q = Q
            self.Q_dense = None
            self.is_sparse = True
        else:
            self.Q = np.asarray(Q, dtype=float)
            # Symmetrize if needed
            if not np.allclose(self.Q, self.Q.T):
                self.Q = (self.Q + self.Q.T) / 2
            self.Q_dense = self.Q
            self.is_sparse = False
        
        self.q = np.asarray(q, dtype=float)
        self.n = len(self.q)
        
        # Validate blocks
        is_valid, error_msg = validate_blocks(blocks, self.n)
        if not is_valid:
            raise ValueError(f"Invalid blocks: {error_msg}")
        self.blocks = blocks
        self.n_blocks = len(blocks)
        
        # Configuration
        self.cfg = self.default_config()
        if cfg is not None:
            self.cfg.update(cfg)
        
        # State variables and Factorization cache initialized to None...

    @staticmethod
    def default_config():
        return {
            'sigma': 0.1,  # Centering parameter in (0, 0.5)
            'max_iter': 100,
            'eps_feas': 1e-8,
            'eps_comp': 1e-8,
            'eps_delta': 1e-8,  # For δ-rule
            'tau_delta': 1e-2,  # For δ-rule
            'tau_reg': None,  # None for adaptive, or fixed value
            'K_th': 200,  # Threshold for assembling Schur matrix
            'pcg_tol': 1e-10,
            'pcg_maxit': 1000,
            'verbosity': 1,
            'gamma': 0.99,  # Fraction-to-boundary safety factor
        }
```

#### Mathematical & Computational Analysis:
1.  **Sparse vs. Dense Support (`is_sparse`)**: Unlike the naive baseline, this solver is built from the ground up to handle both dense `np.ndarray` and `scipy.sparse` matrices for $Q$. This is a massive computational upgrade. If $Q$ is sparse, matrix-vector products ($Qx$) drop from $O(n^2)$ to $O(\text{nnz})$, where $\text{nnz}$ is the number of non-zero elements. While `feedback2.tex` formulates the problem purely mathematically ($Q \in \mathbb{R}^{n \times n}$), supporting both data structures bridges the gap between pure math and high-performance computing.
2.  **Symmetrization**: If a dense $Q$ is provided, the code explicitly symmetrizes it: `(Q + Q.T) / 2`. As stated in `feedback2.tex`, the formulation requires $Q$ to be symmetric positive semidefinite. If a user passes a matrix $Q$ that is slightly asymmetric due to floating-point errors, the KKT conditions (specifically the gradient $\nabla(\frac{1}{2}x^T Q x) = Qx$) become mathematically invalid. Symmetrizing it upfront is a standard defensive programming practice.
3.  **Block Validation**: It immediately calls `validate_blocks` (from `block_ops.py`) to ensure the partition $I^k$ is mathematically sound before doing any work.
4.  **The Configuration (`default_config`)**:
    *   `sigma` ($\sigma = 0.1$): This is the centering parameter from `feedback2.tex`. It dictates how aggressively the algorithm pushes towards the central path ($\mu \to 0$). A fixed $\sigma \in (0, 0.5)$ is standard for short-step feasible methods.
    *   `tau_reg`: This controls Tikhonov regularization ($\tau I$) for the Newton matrix $M$. Setting it to `None` enables *adaptive* regularization, avoiding the $O(n^3)$ condition number checks discussed in `utils.py`.
    *   `K_th` (Threshold for Schur): This is a very interesting computational choice. If the number of blocks $|K|$ is small ($\le 200$), it will build the dense Schur complement matrix $S$ and use Cholesky factorization. If $|K| > 200$, it switches to a matrix-free Preconditioned Conjugate Gradient (PCG) solver. This perfectly balances the overhead of iterative solvers vs. the memory limits of dense matrices.

## 2. Initialization: The $\delta$-Rule

### 2.1 `initialize_delta_rule`
```python
    def initialize_delta_rule(self):
        """
        Initialize strictly interior point via δ-rule.
        """
        # ==========================================
        # Step 1.1: Uniform primal (x^0_i = 1/|I_k| for i in I_k)
        # ==========================================
        x = np.zeros(self.n, dtype=float)
        for k, block in enumerate(self.blocks):
            block_size = len(block)
            if block_size == 0:
                raise ValueError(f"Block {k} is empty")
            x[block] = 1.0 / block_size
```
**Analysis (Step 1.1):**
*   **Theory (`feedback2.tex`)**: The algorithm is a *feasible-start* IPM. This means the initial point $(x^0, y^0, z^0)$ must strictly satisfy the primal constraints ($Ex = \mathbf{1}$, $x > 0$) and dual constraints ($Qx + q + E^T y - z = 0$, $z > 0$).
*   **Computational Choice**: To satisfy $Ex = \mathbf{1}$ and $x > 0$, the simplest and most robust choice is the barycenter of each simplex. For a block $k$ with $|I^k|$ variables, setting each variable to $1 / |I^k|$ guarantees their sum is exactly 1 and they are strictly positive. This is computationally trivial ($O(n)$) and perfectly robust.

```python
        # ==========================================
        # Step 1.2: Compute w = Q x^0 + q
        # ==========================================
        if self.is_sparse:
            w = self.Q.dot(x) + self.q
        else:
            w = self.Q @ x + self.q
```
**Analysis (Step 1.2):**
*   **Theory**: We need to find $y$ and $z$ such that $Qx + q + E^T y - z = 0$. Let's define the gradient of the objective at $x^0$ as $w = Qx^0 + q$. The dual constraint becomes $w + E^T y - z = 0$, or equivalently, $z = w + E^T y$.
*   **Computational Choice**: This explicitly handles the sparse vs. dense matrix multiplication we discussed in `__init__`. `self.Q.dot(x)` uses SciPy's optimized sparse matrix-vector product, which is $O(\text{nnz})$ instead of $O(n^2)$.

```python
        # ==========================================
        # Step 1.3: Compute per-block δ_k
        # ==========================================
        delta_per_block = np.zeros(self.n_blocks, dtype=float)
        for k, block in enumerate(self.blocks):
            w_block = w[block]
            w_max = np.max(w_block)
            w_min = np.min(w_block)
            w_range = w_max - w_min
            delta_k = max(self.cfg['eps_delta'], self.cfg['tau_delta'] * w_range)
            delta_per_block[k] = delta_k
```
**Analysis (Step 1.3):**
*   **Theory**: We know $z = w + E^T y$. The operator $E^T y$ simply broadcasts the scalar $y_k$ to all variables in block $k$. So, for a specific block $k$, the dual variables are $z_{I^k} = w_{I^k} + y_k \mathbf{1}$. We require $z > 0$, which means $w_{I^k} + y_k > 0$, or $y_k > -\min(w_{I^k})$. To ensure *strict* positivity, we add a margin $\delta_k > 0$, setting $y_k = -\min(w_{I^k}) + \delta_k$.
*   **Computational Choice (The "Per-Block" Upgrade)**: This is a massive upgrade over the naive baseline (`algo1_baseline_solver.py`), which likely used a single, global $\delta$ for all blocks. 
    *   Why is a global $\delta$ bad? Imagine block 1 has gradient values $w_{I^1} \approx 10^6$, and block 2 has $w_{I^2} \approx 10^{-2}$. If you use a global $\delta = 1$, it's negligible for block 1 but massive for block 2. This artificially inflates $z$ for block 2, which inflates the complementarity gap $\mu = (x^T z)/n$, forcing the solver to do many unnecessary iterations to shrink $\mu$ back down.
    *   Here, $\delta_k$ is scaled relative to the *range* of the gradient within that specific block (`w_range = w_max - w_min`). `tau_delta` (default 0.01) ensures $\delta_k$ is 1% of the block's gradient spread. `eps_delta` (default $10^{-8}$) provides a floor in case the gradient is perfectly flat in that block. This ensures numerical stability without artificially inflating $\mu$.

```python
        # ==========================================
        # Step 1.4: Dual start (y_k = -min(w_Ik) + δ_k, z = w + E^T y)
        # ==========================================
        # Coherent implementation using helper
        y = np.zeros(self.n_blocks, dtype=float)
        
        for k, block in enumerate(self.blocks):
            w_block = w[block]
            w_min = np.min(w_block)
            delta_k = delta_per_block[k]
            y[k] = -w_min + delta_k
            
        # Now use the helper to compute z = w + E^T y
        Ety = apply_E_transpose(y, self.blocks, self.n)
        z = w + Ety
```
**Analysis (Step 1.4):**
*   **Computational Choice (Readability vs. Micro-optimization)**: The original implementation of this step used a single loop to compute both `y[k]` and `z[block]` simultaneously:
    ```python
    # Old approach (single pass)
    y = np.zeros(self.n_blocks, dtype=float)
    z = np.zeros(self.n, dtype=float)
    for k, block in enumerate(self.blocks):
        w_block = w[block]
        w_min = np.min(w_block)
        y[k] = -w_min + delta_per_block[k]
        z[block] = w[block] + y[k]
    ```
    While the old approach was a clever micro-optimization (avoiding a second $O(n)$ pass over the blocks), it sacrificed readability and coherence with the rest of the codebase. The current implementation explicitly uses the `apply_E_transpose` helper function. This directly translates the mathematical formula $z = w + E^T y$ into code, making it immediately obvious what is happening. Since this initialization only happens *once* at the very beginning of the solver, saving a fraction of a millisecond by avoiding a second loop is irrelevant compared to the clarity gained by using the established matrix-free operators.

```python
        # ==========================================
        # Step 1.5: Compute initial gap and Sanity Checks
        # ==========================================
        mu = np.dot(x, z) / self.n
        
        # Sanity checks
        if np.any(x <= 0): raise ValueError("Initial x is not strictly positive")
        if np.any(z <= 0): raise ValueError("Initial z is not strictly positive")
        
        # Check feasibility (should be exact for uniform x)
        Ex = apply_E(x, self.blocks)
        if not np.allclose(Ex, 1.0, atol=1e-10):
            raise ValueError(f"Initial x does not satisfy Ex = 1: {Ex}")
        
        # Check dual feasibility (should be approximately satisfied)
        Ety = apply_E_transpose(y, self.blocks, self.n)
        r_D = w + Ety - z
        if norm_inf(r_D) > 1e-6:
            if self.cfg['verbosity'] >= 2:
                print(f"Warning: Initial dual residual is large: {norm_inf(r_D)}")
        
        self.x = x; self.y = y; self.z = z; self.mu = mu
        return x, y, z, mu
```
**Analysis (Step 1.5):**
*   **Theory**: The complementarity gap $\mu$ is defined as $\frac{1}{n} \sum_{i=1}^n x_i z_i$. Because $x > 0$ and $z > 0$, $\mu$ is strictly positive. The goal of the IPM is to drive $\mu \to 0$ while maintaining feasibility.
*   **Computational Choice (Sanity Checks)**: This is excellent defensive programming. Before starting the expensive Newton iterations, it explicitly verifies the fundamental assumptions of the feasible-start IPM:
    1.  $x > 0$ and $z > 0$.
    2.  Primal feasibility: $Ex = \mathbf{1}$ (using the $O(n)$ `apply_E` helper).
    3.  Dual feasibility: $Qx + q + E^T y - z = 0$ (using `apply_E_transpose` and `norm_inf` from `utils.py`).
    If any of these fail, the algorithm will mathematically collapse later, so it's better to catch it here.

## 3. The Main Loop: Residuals and Factorization

### 3.1 `compute_residuals`
This function is called at the start of every iteration to check how far the current point $(x, y, z)$ is from satisfying the KKT conditions.

```python
    def compute_residuals(self):
        """
        Compute primal, dual, and complementarity residuals.
        """
        # Dual residual: r_D = Qx + q + E^T y - z
        if self.is_sparse:
            Qx = self.Q.dot(self.x)
        else:
            Qx = self.Q @ self.x
        Ety = apply_E_transpose(self.y, self.blocks, self.n)
        r_D = Qx + self.q + Ety - self.z
```
**Analysis (Dual Residual):**
*   **Theory (`feedback2.tex`)**: The dual feasibility condition is $Qx + q + E^T y - z = 0$. The residual $r_D$ measures the violation of this condition.
*   **Computational Choice (Sparsity)**: The dual residual requires computing $Qx$. If $Q$ is a dense $10,000 \times 10,000$ matrix, `Q @ x` requires $10^8$ floating-point operations. However, if $Q$ is sparse (e.g., a graph Laplacian), it might only have $50,000$ non-zero elements. The `if self.is_sparse:` block explicitly uses `self.Q.dot(x)`, which is SciPy's optimized sparse matrix-vector product. This reduces the computational complexity of this step from $O(n^2)$ to $O(\text{nnz})$, which is a massive speedup for large, sparse problems.
*   **Computational Choice (Matrix-Free $E^T$)**: Instead of building a dense $E^T$ matrix and doing `E.T @ y` (which would be $O(K \cdot n)$), it uses the `apply_E_transpose` helper. This broadcasts the dual variables $y$ to the full dimension $n$ in exactly $O(n)$ time.

```python
        # Primal residual: r_P = Ex - 1
        Ex = apply_E(self.x, self.blocks)
        r_P = Ex - 1.0
```
**Analysis (Primal Residual):**
*   **Theory**: The primal feasibility condition is $Ex - \mathbf{1} = 0$. The residual $r_P$ measures the violation of the simplex constraints.
*   **Computational Choice (Matrix-Free $E$)**: Just like the dual residual, this avoids dense matrix multiplication. It uses the `apply_E` helper to sum the variables within each block in $O(n)$ time. Combined with the sparse $Qx$, the entire linear residual computation is extremely fast and memory-efficient compared to the naive baseline.

```python
        # Complementarity residual: r_C = XZ 1 - μ_target 1
        # Note: In feasible-start, we want to drive XZ to σμ
        mu_target = self.cfg['sigma'] * self.mu
        XZ = self.x * self.z
        r_C = XZ - mu_target
        
        return r_D, r_P, r_C, mu_target
```
**Analysis (Complementarity Residual):**
*   **Theory**: The strict complementarity condition is $X Z \mathbf{1} = 0$ (where $X = \text{diag}(x)$, $Z = \text{diag}(z)$). Because this is an Interior-Point Method, we don't target $0$ directly, as that would push variables to the boundary too fast and the algorithm would get stuck. Instead, we target a point on the "central path": $X Z \mathbf{1} = \sigma \mu \mathbf{1}$, where $\mu$ is the current average gap and $\sigma \in (0,1)$ is the centering parameter.
*   **Computational Choice**: The code explicitly calculates `mu_target = self.cfg['sigma'] * self.mu`. The complementarity residual is then $r_C = x \odot z - \mu_{\text{target}}$. This is exactly the right-hand side needed for the Newton step equations. The element-wise multiplication `self.x * self.z` is a fast $O(n)$ operation.

### 3.2 `build_H_and_factorize`
This function constructs the core Newton matrix $H = Q + X^{-1}Z$ and factorizes it so that systems of the form $H \Delta x = \text{rhs}$ can be solved quickly. We break this down into three critical phases: Construction, Regularization, and Factorization.

#### Phase 1: Constructing the Newton Matrix $H$
```python
        # Safe division to avoid ZeroDivisionError
        x_safe = np.maximum(self.x, 1e-14)
        x_inv_z = self.z / x_safe
        
        if self.is_sparse:
            X_inv_Z = scipy.sparse.diags(x_inv_z, format='csc')
            H = self.Q + X_inv_Z
        else:
            H = self.Q + np.diag(x_inv_z)
```
**Mathematical & Computational Analysis:**
*   **Theory (`feedback2.tex`)**: The Newton step requires solving a system involving the matrix $H = Q + X^{-1}Z$. Here, $X = \text{diag}(x)$ and $Z = \text{diag}(z)$. Because $x > 0$ and $z > 0$, and $Q$ is positive semidefinite, $H$ is theoretically symmetric positive definite (SPD) and invertible.
*   **Nuance (Symmetry vs M formulation)**: The paper also mentions an alternative formulation $M = Z + XQ$. While $M$ avoids explicit division by $x$, it results in an **asymmetric matrix** (since the product of a diagonal matrix and a symmetric matrix is not symmetric). By using $H$, we guarantee symmetry, which allows us to use blazing-fast symmetric solvers like Cholesky factorization. To prevent division by zero as $x \to 0$, we use a safe division threshold (`1e-14`).
*   **Computational Choice (Sparse vs Dense)**: For the sparse path, we use `scipy.sparse.diags` to create sparse diagonal matrices and use the `csc` (Compressed Sparse Column) format.

#### Phase 2: The Regularization Heuristic
```python
            # (Sparse and Dense paths share this logic)
            tau = self.cfg['tau_reg']
            if tau is None:
                # Adaptive regularization via infinity norm
                if self.is_sparse:
                    H_norm_inf = np.max(np.abs(H).sum(axis=1).A1)
                else:
                    H_norm_inf = norm_inf(H)
                tau = max(0.0, min(1e-8, 1e-10 * H_norm_inf))
            
            if tau > 0.0:
                # Add tau * I
                if self.is_sparse:
                    H = H + tau * scipy.sparse.eye(self.n, format='csc')
                else:
                    H = H + tau * np.eye(self.n)
                self.H_reg = tau
```
**Mathematical & Computational Analysis:**
*   **Theory (Ill-conditioning)**: As the Interior-Point Method converges, the complementarity gap $\mu \to 0$. This means for every index $i$, either $x_i \to 0$ or $z_i \to 0$. Consequently, the matrix $H = Q + X^{-1}Z$ develops entries of wildly different magnitudes, causing its condition number to blow up. A highly ill-conditioned matrix cannot be factorized stably due to floating-point roundoff errors.
*   **Nuance (Why Infinity Norm?)**: To fix ill-conditioning, we apply Tikhonov regularization: $H \leftarrow H + \tau I$. But how do we choose $\tau$? 
    *   *The Naive Way*: Compute the exact condition number using Singular Value Decomposition (SVD). If it's bad, apply $\tau$. However, SVD is an $O(n^3)$ operation for dense matrices and practically impossible for large sparse matrices.
    *   *The Smart Way (Implemented)*: We use a fast heuristic. We estimate the "size" of the matrix using the infinity norm $||H||_\infty$ (which is simply the maximum absolute row sum, an $O(n)$ operation). We then set $\tau$ to be a tiny fraction of this norm ($\approx 10^{-10} ||H||_\infty$), capped at $10^{-8}$. This provides just enough regularization to stabilize the matrix without distorting the Newton direction, and it computes almost instantly.

#### Phase 3: Factorization and the "Try-Catch" Fallback
```python
            # Sparse Path Factorization
            try:
                self.H_factor = scipy.sparse.linalg.splu(H_csc, permc_spec='MMD_AT_PLUS_A')
            except Exception:
                bump = max(1e-12, (self.H_reg or 0.0) * 10 or 1e-12)
                H_csc = H_csc + bump * scipy.sparse.eye(self.n, format="csc")
                self.H_reg = (self.H_reg or 0.0) + bump
                self.H_factor = scipy.sparse.linalg.splu(H_csc, permc_spec="MMD_AT_PLUS_A")
            
            # Dense Path Factorization uses cho_factor(H) with the exact same try-except logic
```
**Mathematical & Computational Analysis:**
*   **Computational Choice (Cholesky)**: Because $H$ is symmetric positive definite (as established in Phase 1), we can use the extremely fast Cholesky factorization ($LL^T$) for dense matrices via `cho_factor`. For sparse matrices, since SciPy lacks a built-in sparse Cholesky without external dependencies, we use `splu` with a symmetric permutation (`MMD_AT_PLUS_A`).
*   **Nuance (The Try-Catch Bump)**: Why use a `try...except` block for math? This is a pragmatic optimization technique. We want to use the *minimum* possible regularization $\tau$ to keep the Newton step as accurate as possible. Sometimes, our $O(n)$ heuristic from Phase 2 underestimates the required $\tau$, and the factorization fails (raises an Exception due to a non-positive-definite matrix). Instead of doing expensive checks upfront, we simply *try* to factorize it. If it fails, we catch the error, aggressively bump $\tau$ by a factor of 10, and try again. This "optimistic" approach is much faster on average than being overly cautious.

#### Phase 4: The Dense Path Differences
While the logical flow (Construct $\to$ Regularize $\to$ Factorize) is identical for both sparse and dense matrices, the underlying computational tools differ significantly:
*   **Construction**: The dense path uses standard NumPy arrays (`np.diag`) and dense matrix multiplication (`@`), which is $O(n^3)$ but highly optimized via BLAS/LAPACK. The sparse path uses `scipy.sparse.diags` and sparse matrix multiplication, which is $O(\text{nnz})$.
*   **Regularization**: The dense path uses the `norm_inf` helper from `utils.py` (which calls `np.linalg.norm(M, ord=np.inf)`). The sparse path manually computes the maximum absolute row sum (`np.max(np.abs(M).sum(axis=1).A1)`) because `scipy.sparse` matrices do not support `np.linalg.norm` directly in the same way.
*   **Factorization**: The dense path uses `scipy.linalg.lu_factor`, which returns a tuple `(lu, piv)` representing the LU decomposition and pivot indices. The sparse path uses `scipy.sparse.linalg.splu`, which returns a `SuperLU` object. This difference dictates how the subsequent `solve_M_system` function must be implemented (using `lu_solve` vs `SuperLU.solve`).

### 3.3 `solve_M_system`
This function is a small but mathematically critical utility used repeatedly during the Schur complement phase.

```python
    def solve_M_system(self, rhs):
        """
        Solve M * sol = rhs using cached factorization.
        """
        rhs = np.asarray(rhs, dtype=float)
        
        if self.is_sparse:
            # Sparse solve
            return self.M_factor.solve(rhs)
        else:
            # Dense solve via LU
            return lu_solve(self.M_factor, rhs)
```

#### Mathematical & Computational Analysis:
*   **Theory (`feedback2.tex`)**: To find the Newton step $(\Delta x, \Delta y, \Delta z)$, the algorithm performs block elimination on the KKT system. This elimination requires applying the inverse of the Newton matrix, $M^{-1}$, to various right-hand side vectors (e.g., computing $M^{-1} r_C$).
*   **Mathematical Nuance (Never Invert)**: A fundamental rule of numerical linear algebra is that you **never explicitly compute the inverse matrix $M^{-1}$**. Computing an explicit inverse is $O(n^3)$, destroys sparsity (the inverse of a sparse matrix is almost always dense), and is highly susceptible to floating-point errors. Instead, computing $v = M^{-1} \text{rhs}$ is treated as solving the linear system $M v = \text{rhs}$ for the vector $v$.
*   **Computational Choice (Forward/Backward Substitution)**: Because we already decomposed $M$ into Lower ($L$) and Upper ($U$) triangular matrices in `build_M_and_factorize` ($M = LU$), solving $M v = \text{rhs}$ becomes $L(Uv) = \text{rhs}$. This is solved in two lightning-fast steps:
    1.  Forward substitution: Solve $Ly = \text{rhs}$ for $y$.
    2.  Backward substitution: Solve $Uv = y$ for $v$.
    Because $L$ and $U$ are triangular, this process drops from $O(n^3)$ to $O(n^2)$ for dense matrices, and down to roughly $O(\text{nnz})$ for sparse matrices. This is exactly why `self.M_factor` was cached.
*   **Code Nuance (API Differences)**: The code handles the differing APIs of SciPy's sparse and dense libraries:
    *   **Sparse**: `self.M_factor` is a `SuperLU` object. It has a built-in `.solve()` method that handles the substitution internally using optimized C code.
    *   **Dense**: `self.M_factor` is a tuple `(lu, piv)` containing the packed LU matrix and pivot indices. It must be passed to the separate `scipy.linalg.lu_solve` function, which wraps the highly optimized LAPACK routine `dgetrs`.

## 4. The Schur Complement System

The core of the algorithm's efficiency lies in how it solves the Newton step. Instead of solving the massive $3n \times 3n$ KKT system directly, it reduces it to a much smaller $|K| \times |K|$ system for the dual variables $\Delta y$, known as the Schur complement system: $S \Delta y = b$.

### 4.1 `schur_rhs`
```python
    def schur_rhs(self, r_P, r_D, r_C):
        """
        Compute Schur system RHS for S Δy = b with S = E M^{-1} X E^T:
            b = r_P + E M^{-1} (r_C + X r_D)
        """
        # Core RHS for the M^{-1} application: r_C + X r_D
        if r_D is None:
            rhs_core = r_C
        else:
            rhs_core = r_C + self.x * r_D
        
        H_inv_rhs = self.solve_H_system(rhs_core)
        
        # b = r_P - E H^{-1} rhs_core  (ensures E Δx = -r_P)
        b = r_P - apply_E(H_inv_rhs, self.blocks)
        return b
```

#### Mathematical & Computational Analysis:
*   **Theory (`feedback2.tex`)**: By eliminating $\Delta x$ and $\Delta z$ from the KKT conditions, we arrive at the Schur complement equation $S \Delta y = b$. The right-hand side $b$ is mathematically derived as $r_P - E H^{-1} (r_D + X^{-1} r_C)$.
*   **Numerical Robustness ($r_D$)**: In a pure feasible-start method, the dual residual $r_D$ should theoretically be zero. However, due to floating-point arithmetic, small errors accumulate over iterations, causing the iterates to drift away from the feasible manifold. By explicitly including $r_D$ in the RHS computation (`rhs_core = r_D + x_inv_r_C`), the algorithm actively corrects for this drift, pulling the iterates back to feasibility. This is a crucial difference between a theoretical algorithm and a robust numerical implementation.
*   **Computational Choice (Matrix-Free $E$)**: Notice the operation `apply_E(H_inv_rhs, self.blocks)`. Instead of constructing the $|K| \times n$ matrix $E$ and performing a dense matrix-vector multiplication, it uses the $O(n)$ block operator.

### 4.2 `schur_operator` and Matrix-Free Block Operations
```python
    def schur_operator(self, v):
        """
        Apply Schur operator: S v = E * H^{-1} * (E^T v).
        """
        v = np.asarray(v, dtype=float)
        
        # Step 1: E^T v
        Ety = apply_E_transpose(v, self.blocks, self.n)
        
        # Step 2: H^{-1} * (E^T v)
        H_inv_Ety = self.solve_H_system(Ety)
        
        # Step 3: E * H^{-1} * (E^T v)
        Sv = apply_E(H_inv_Ety, self.blocks)
        
        return Sv
```

#### Mathematical & Computational Analysis:
*   **Theory**: The Schur complement matrix is $S = E H^{-1} E^T$. To solve $S \Delta y = b$ using an iterative method like Preconditioned Conjugate Gradient (PCG), we don't actually need the matrix $S$ itself; we only need a function that computes the matrix-vector product $S v$ for any given vector $v$. This is exactly what `schur_operator` does.
*   **The Matrix-Free Engine (`apply_E` and `apply_E_transpose`)**: This function is the perfect showcase of why the `block_ops.py` helpers are so powerful. Let's break down the complexity of computing $S v$:
    1.  **`Ety = apply_E_transpose(v, ...)`**: Mathematically, $E^T$ is an $n \times |K|$ matrix. A dense multiplication $E^T v$ would take $O(n \times |K|)$ operations and require storing $E^T$. However, $E^T$ simply maps a block-level value to all variables within that block. `apply_E_transpose` achieves this by simply broadcasting the values in $O(n)$ time with zero matrix allocations.
    2.  **`H_inv_Ety = self.solve_H_system(Ety)`**: As analyzed previously, this uses the cached Cholesky factors to perform forward/backward substitution in $O(n^2)$ (dense) or $O(\text{nnz})$ (sparse) time, completely avoiding the $O(n^3)$ explicit inversion.
    3.  **`Sv = apply_E(H_inv_Ety, ...)`**: Mathematically, $E$ is a $|K| \times n$ matrix. A dense multiplication would be $O(|K| \times n)$. But $E$ simply sums the values within each block. `apply_E` does exactly this using `np.add.reduceat` in $O(n)$ time.
*   **The Grand Total**: By chaining these matrix-free operations, the entire action of the Schur complement $S v$ is computed in $O(\text{nnz})$ time (dominated by the `solve_H_system` step) without *ever* building $E$, $E^T$, $H^{-1}$, or $S$ in memory. This is the secret sauce that allows the solver to scale to massive dimensions.

## 5. Solving the System and Recovering the Step

### 5.1 `schur_solve`: The Hybrid Solver

This function is responsible for solving the Schur complement system: $S \Delta y = b$.

#### The Mathematical Context (`feedback2.tex`)
By eliminating $\Delta x$ and $\Delta z$ from the KKT conditions, we arrive at the reduced system for the dual variables $\Delta y$:
$$ (E H^{-1} E^T) \Delta y = b $$
Where $S = E H^{-1} E^T$. Because $H = Q + X^{-1}Z$ is positive definite (thanks to our regularization and $Q \succeq 0$), the matrix $S$ is mathematically guaranteed to be **Symmetric Positive Definite (SPD)**.

#### The Baseline Approach (`algo1_baseline_solver.py`)
```python
# From algo1_baseline_solver.py
H_inv_ET  = np.linalg.solve(H, E.T)  # O(K * n^3)
S = E @ H_inv_ET                     # O(K^2 * n)
dy = np.linalg.solve(S, b)           # O(K^3)
```
The baseline explicitly builds the dense matrix $S$ by solving $K$ dense linear systems of size $n \times n$. It then uses `np.linalg.solve`, which performs a standard LU decomposition, completely ignoring the fact that $S$ is SPD. This is computationally disastrous for large $n$ or large $K$.

#### The Final Solver Approach (`algo1_final_solver.py`)

The final solver uses a dynamic hybrid approach based on the number of blocks $|K|$ (`self.cfg['K_th']`, default 200).

**Path A: Direct Solver (Small $|K|$)**
```python
        if assemble_if_small and self.n_blocks <= self.cfg['K_th']:
            # Assemble full Schur matrix S
            S = np.zeros((self.n_blocks, self.n_blocks), dtype=float)
            for j in range(self.n_blocks):
                e_j = np.zeros(self.n_blocks, dtype=float)
                e_j[j] = 1.0
                S[:, j] = self.schur_operator(e_j)
```
*   **Analysis**: If the number of blocks is small, it is efficient to build the dense matrix $S$. However, instead of the baseline's massive dense matrix multiplications, it builds $S$ column-by-column. It creates a standard basis vector $e_j$ (all zeros, with a 1 at index $j$) and passes it through our $O(n)$ `schur_operator`. The output is exactly the $j$-th column of $S$.

```python
            # Factorize and solve
            try:
                S_factor = cho_factor(S, lower=False)
                d_y = cho_solve(S_factor, b)
            except np.linalg.LinAlgError:
                # If Cholesky fails, use regularized version
                S_reg = S + 1e-12 * np.eye(self.n_blocks)
                S_factor = cho_factor(S_reg, lower=False)
                d_y = cho_solve(S_factor, b)
```
*   **Analysis**: Because $S$ is SPD, the code uses `scipy.linalg.cho_factor` (Cholesky decomposition, $S = LL^T$). Cholesky is **twice as fast** as the baseline's LU solver and uses half the memory. 
*   **Robustness**: If numerical roundoff causes $S$ to lose its positive definiteness (raising a `LinAlgError`), it catches the error, adds a tiny Tikhonov regularization ($10^{-12} I$), and tries again. Note that this $10^{-12}$ regularization is applied directly to the Schur matrix $S$, which is completely separate from the $\tau$ regularization applied to the Newton matrix $M$ in `build_M_and_factorize`.

**Path B: Iterative Solver (Large $|K|$)**
```python
        else:
            # Use PCG on operator
            def S_op(v):
                return self.schur_operator(v)
            
            M_precond = None  # Identity preconditioner
            
            # Solve with PCG
            d_y, info = scipy.sparse.linalg.cg(
                scipy.sparse.linalg.LinearOperator(
                    (self.n_blocks, self.n_blocks), matvec=S_op, dtype=float
                ),
                b, tol=self.cfg['pcg_tol'], maxiter=self.cfg['pcg_maxit'], M=M_precond
            )
```
*   **Analysis**: If $|K|$ is large (e.g., thousands of blocks), assembling $S$ would consume too much memory and time. Instead, it uses `scipy.sparse.linalg.cg` (Conjugate Gradient). CG is an iterative solver designed specifically for SPD systems. 
*   **The Matrix-Free Magic**: Notice the `LinearOperator`. CG *never* asks for the matrix $S$. It only asks: "If I give you a vector $v$, what is $S \times v$?". We provide `S_op`, which simply calls our matrix-free `schur_operator`. This allows the solver to find $\Delta y$ using only $O(n)$ memory and $O(\text{nnz})$ operations per iteration. This is the ultimate scaling mechanism that the baseline completely lacks.

### 5.2 `back_substitute_dx_dz`: Recovering the Step

Once $\Delta y$ is found, we must recover the primal step $\Delta x$ and the dual slack step $\Delta z$.

#### The Mathematical Context (`feedback2.tex`)
From the KKT conditions, the formulas for back-substitution are:
1. $\Delta x = H^{-1} (-X^{-1} r_C - r_D - E^T \Delta y)$
2. $\Delta z = Q \Delta x + E^T \Delta y$

#### The Baseline Approach (`algo1_baseline_solver.py`)
```python
# From algo1_baseline_solver.py
dx = np.linalg.solve(H, rhs - E.T @ dy)
dz = rD + Q @ dx + E.T @ dy
```
*   **The Flaw**: The baseline calls `np.linalg.solve(H, ...)` *again*. This means it performs a completely new $O(n^3)$ matrix inversion from scratch, throwing away all the computational work it did earlier when building the Schur complement. It also uses dense matrix multiplication `E.T @ dy`.

#### The Final Solver Approach (`algo1_final_solver.py`)

```python
    def back_substitute_dx_dz(self, r_C, d_y, r_D=None):
        # Safe division
        x_safe = np.maximum(self.x, 1e-14)
        x_inv_r_C = r_C / x_safe
        
        # Step 1: E^T * Δy
        Ety_dy = apply_E_transpose(d_y, self.blocks, self.n)
```
*   **Analysis**: Instead of the baseline's dense `E.T @ dy`, it uses the $O(n)$ `apply_E_transpose` helper.

```python
        # Step 2: Δx = H^{-1} * (-X^{-1} r_C - r_D - E^T * Δy)
        rhs_dx = -x_inv_r_C - Ety_dy
        if r_D is not None:
            rhs_dx = rhs_dx - r_D
        d_x = self.solve_H_system(rhs_dx)
```
*   **Analysis (Reusing the Engine)**: To apply $H^{-1}$, it calls `self.solve_H_system(rhs_dx)`. Because we cached the Cholesky factorization of $H$ back in `build_H_and_factorize`, this step drops from an $O(n^3)$ inversion to a simple $O(n^2)$ (or $O(\text{nnz})$) forward/backward substitution. It is nearly instantaneous.
*   **Analysis (Numerical Robustness)**: Notice the inclusion of `- r_D`. In pure math (`feedback2.tex`), $r_D = 0$ for a feasible-start method. In code, floating-point drift happens. Including $r_D$ acts as a feedback loop, forcing the Newton step to pull the iterates back to exact feasibility.

```python
        # Step 4: Δz = Q * Δx + E^T * Δy + r_D
        if self.is_sparse:
            Q_dx = self.Q.dot(d_x)
        else:
            Q_dx = self.Q @ d_x
        d_z = Q_dx + Ety_dy
        if r_D is not None:
            d_z = d_z + r_D
        
        return d_x, d_z
```
*   **Analysis**: Finally, it computes $\Delta z$. It explicitly handles the sparse vs. dense matrix multiplication for $Q \Delta x$, reuses the already computed `Ety_dy`, and again includes the dual residual $r_D$ for numerical stability.

## 6. State Updates and Convergence Checking

After computing the Newton step $(\Delta x, \Delta y, \Delta z)$ and determining the appropriate step size $\alpha$, the algorithm must update its internal state and check if it has reached the optimal solution.

### 6.1 `update_and_mu`

```python
    def update_and_mu(self, alpha, d_x, d_y, d_z):
        """
        Update iterates and compute new complementarity gap.
        """
        self.x = self.x + alpha * d_x
        self.y = self.y + alpha * d_y
        self.z = self.z + alpha * d_z
        self.mu = np.dot(self.x, self.z) / self.n
```

**Mathematical & Computational Analysis:**
*   **The Update Rule (`feedback2.tex`)**: This is the direct implementation of the standard Interior-Point Method update rule: $v^{k+1} = v^k + \alpha \Delta v$. Because $\alpha$ is computed using the fraction-to-boundary rule (which we will see in `iterate_once`), the math guarantees that the new `self.x` and `self.z` remain strictly positive ($> 0$).
*   **The Baseline Comparison (`algo1_baseline_solver.py`)**:
    ```python
    # From algo1_baseline_solver.py
    x += alpha * dx
    y += alpha * dy
    z += alpha * dz
    x = np.maximum(x, tiny)  # tiny = 1e-16
    z = np.maximum(z, tiny)
    ```
    *   **The Baseline's Flaw**: The baseline performs the same update, but then it applies a highly destructive hack: `np.maximum(x, tiny)`. It artificially forces variables to be at least $10^{-16}$. 
    *   **Why the Final Solver is Better**: The final solver *trusts the math*. It does not use `np.maximum`. Forcing variables to a hard floor of $10^{-16}$ breaks the KKT conditions. If the true optimal solution requires $x_i = 10^{-20}$, forcing it to $10^{-16}$ creates an artificial gradient that fights the Newton step, often causing the baseline solver to stall and fail to converge. The final solver avoids this entirely.
*   **Recomputing $\mu$**: The complementarity gap is defined as $\mu = \frac{x^T z}{n}$. The final solver computes this immediately after the update using a fast $O(n)$ dot product (`np.dot`). Because $x > 0$ and $z > 0$, $\mu$ is guaranteed to be strictly positive. This $\mu$ is the primary metric for how close the algorithm is to the optimal solution.

### 6.2 `converged`

```python
    def converged(self, r_D, r_P):
        """
        Check convergence.
        Converged if:
        - ||r_P||_∞ <= ε_feas
        - ||r_D||_∞ <= ε_feas
        - μ <= ε_comp
        """
        norm_r_P = norm_inf(r_P)
        norm_r_D = norm_inf(r_D)
        
        feas_ok = norm_r_P <= self.cfg['eps_feas'] and norm_r_D <= self.cfg['eps_feas']
        comp_ok = self.mu <= self.cfg['eps_comp']
        
        return feas_ok and comp_ok
```

**Mathematical & Computational Analysis:**
*   **The Termination Criteria (`feedback2.tex`)**: The algorithm terminates when the current point $(x, y, z)$ satisfies the KKT conditions within a specified tolerance. The three conditions are:
    1.  **Primal Feasibility**: $||Ex - \mathbf{1}||_\infty \le \epsilon_{\text{feas}}$ (measured by `norm_r_P`).
    2.  **Dual Feasibility**: $||Qx + q + E^T y - z||_\infty \le \epsilon_{\text{feas}}$ (measured by `norm_r_D`).
    3.  **Complementarity**: $\mu = \frac{x^T z}{n} \le \epsilon_{\text{comp}}$ (measured by `comp_ok`).
*   **Computational Choice (`norm_inf`)**: The code uses the `norm_inf` helper from `utils.py` to compute the infinity norm ($L_\infty$). The infinity norm simply finds the maximum absolute value in the vector. This is an $O(n)$ operation and is the standard norm used for checking tolerances in optimization solvers because it guarantees that *every single constraint* is satisfied within the tolerance, not just the average.
*   **The Baseline Comparison (`algo1_baseline_solver.py`)**:
    ```python
    # From algo1_baseline_solver.py
    xz_max = float(np.max(x * z))
    if (info["rP_inf"][-1] <= eps_feas and
        info["rD_inf"][-1] <= eps_feas and
        (mu <= eps_comp or xz_max <= eps_comp)):
        break
    ```
    *   **The Baseline's Flaw**: The baseline added an extra, non-standard condition: `xz_max <= eps_comp`. This checks the maximum individual complementarity product $x_i z_i$. 
    *   **Why the Final Solver is Better**: While checking `xz_max` is theoretically interesting, it is overly strict in practice. Often, the average gap $\mu$ will reach $10^{-8}$, but a single variable pair might be at $10^{-7}$. The baseline would refuse to terminate, running for thousands of unnecessary iterations just to push that one pair down. The final solver strictly adheres to the standard IPM termination criteria (using only the average $\mu$), which is mathematically sound and prevents the solver from getting stuck in "tail-end" convergence loops.

## 7. The Main Loop: `iterate_once` and `solve`

These two functions orchestrate the entire algorithm, calling the specialized components we analyzed previously in the correct mathematical sequence.

### 7.1 `iterate_once`

This function executes exactly one iteration of the Primal-Dual Interior-Point Method.

```python
    def iterate_once(self):
        # ==========================================
        # Step 2.1: Residual computation
        # ==========================================
        r_D, r_P, r_C, mu_target = self.compute_residuals()
```
*   **Analysis**: The iteration begins by evaluating the current state against the KKT conditions. As analyzed in Section 3.1, this uses the $O(n)$ matrix-free operators (`apply_E`, `apply_E_transpose`) and sparse matrix-vector products to compute the dual ($r_D$), primal ($r_P$), and complementarity ($r_C$) residuals. It also computes the target gap $\mu_{\text{target}} = \sigma \mu$.

```python
        # ==========================================
        # Step 2.2: System assembly
        # ==========================================
        # Build and factorize H = Q + X^{-1}Z
        H_factor, H_reg = self.build_H_and_factorize()
        
        # Compute Schur system RHS: b = -E H^{-1} (r_D + X^{-1} r_C) (and r_P for robustness)
        b = self.schur_rhs(r_P, r_D, r_C)
```
*   **Analysis**: This is the heaviest computational phase. 
    1.  It builds the Newton matrix $H = Q + X^{-1}Z$, applies the adaptive $\tau$ regularization if needed, and computes its Cholesky factorization (Section 3.2).
    2.  It computes the right-hand side $b$ for the Schur complement system. Crucially, it includes the dual residual $r_D$ to correct for floating-point drift, ensuring the algorithm stays on the feasible manifold (Section 4.1).

```python
        # ==========================================
        # Step 2.3: Direction computation
        # ==========================================
        # Solve S Δy = b
        d_y = self.schur_solve(b)
        
        # Back-substitute to get Δx and Δz
        d_x, d_z = self.back_substitute_dx_dz(r_C, d_y, r_D=r_D)
```
*   **Analysis**: 
    1.  It solves the reduced $|K| \times |K|$ Schur system for the dual step $\Delta y$. As analyzed in Section 5.1, this dynamically uses either a direct Cholesky solver (for small $|K|$) or a matrix-free PCG solver (for large $|K|$).
    2.  It recovers the primal step $\Delta x$ and dual slack step $\Delta z$ using $O(n)$ back-substitution, reusing the cached Cholesky factors of $H$ to avoid expensive matrix inversions (Section 5.2).

```python
        # ==========================================
        # Step 2.4: Step-size selection
        # ==========================================
        # Fraction-to-boundary rule
        alpha_pri, alpha_dual, alpha = fraction_to_boundary(
            self.x, d_x, self.z, d_z, gamma=self.cfg['gamma']
        )
```
*   **Analysis**: It calculates the maximum step size $\alpha \in (0, 1]$ that keeps $x > 0$ and $z > 0$. It uses the `fraction_to_boundary` helper from `utils.py` with a safety factor $\gamma = 0.99$. Unlike the baseline, it does *not* artificially force variables to a minimum threshold like $10^{-16}$, preserving the mathematical integrity of the KKT conditions.

```python
        # ==========================================
        # Step 2.5: Update
        # ==========================================
        self.update_and_mu(alpha, d_x, d_y, d_z)
```
*   **Analysis**: It applies the step: $v^{k+1} = v^k + \alpha \Delta v$, and immediately recomputes the new complementarity gap $\mu = \frac{x^T z}{n}$ (Section 6.1).

```python
        # ==========================================
        # Step 2.6: Stopping criterion
        # ==========================================
        # Check convergence
        r_D_new, r_P_new, _, _ = self.compute_residuals()
        
        # KKT consistency checks (diagnostic at high verbosity)
        if self.cfg['verbosity'] >= 3:
            Ety_dy_chk = apply_E_transpose(d_y, self.blocks, self.n)
            eq1 = (self.Q @ d_x + Ety_dy_chk - d_z) + r_D  # ~0
            eq2 = apply_E(d_x, self.blocks) + r_P          # ~0
            eq3 = self.z * d_x + self.x * d_z + r_C        # ~0
            print(f"    Check eq1 (dual lin): {np.linalg.norm(eq1, np.inf):.3e}")
            print(f"    Check eq2 (primal  ): {np.linalg.norm(eq2, np.inf):.3e}")
            print(f"    Check eq3 (compl. ): {np.linalg.norm(eq3, np.inf):.3e}")
            assert np.linalg.norm(eq2, np.inf) <= 1e-8, "E dx != -r_P (check Schur RHS sign)"

        is_converged = self.converged(r_D_new, r_P_new)
```
*   **Analysis**: It computes the *new* residuals at the updated point $(x^{k+1}, y^{k+1}, z^{k+1})$. It then checks if these new residuals and the new $\mu$ satisfy the strict KKT tolerances (Section 6.2).
*   **Diagnostic Nuance (High Verbosity)**: The block under `if self.cfg['verbosity'] >= 3:` is a powerful debugging tool. It explicitly reconstructs the three KKT Newton equations (Dual Linear, Primal Linear, and Complementarity) using the computed steps $(\Delta x, \Delta y, \Delta z)$ and checks if they sum to zero. If the solver is failing due to numerical instability (e.g., the LU factorization was too inaccurate), these checks will immediately reveal which part of the Newton step is mathematically broken. The `assert` statement acts as a hard fail-safe to ensure the primal step strictly respects the simplex constraints.

### 7.2 `solve`

This is the public-facing method that initializes the algorithm and runs the iteration loop.

```python
    def solve(self):
        # Initialize
        self.initialize_delta_rule()
```
*   **Analysis**: It starts by calling the $\delta$-rule initialization (Section 2). This guarantees a strictly feasible starting point ($x^0 > 0$, $z^0 > 0$, $Ex^0 = \mathbf{1}$, $Qx^0 + q + E^T y^0 - z^0 = 0$).

```python
        history = []
        for iter_num in range(1, self.cfg['max_iter'] + 1):
            info = self.iterate_once()
            info['iter'] = iter_num
            history.append(info)
            
            if info['converged']:
                break
```
*   **Analysis**: This is the standard IPM loop. It repeatedly calls `iterate_once` up to `max_iter` times. It collects statistics (residuals, step sizes, $\mu$) into a `history` list. If `iterate_once` reports `converged == True`, the loop breaks early.

```python
        result = {
            'x': self.x.copy(),
            'y': self.y.copy(),
            'z': self.z.copy(),
            'mu': self.mu,
            'iter': len(history),
            'converged': history[-1]['converged'],
            'history': history,
        }
        return result
```
*   **Analysis**: It returns the final optimal point $(x^*, y^*, z^*)$ along with the convergence history. It uses `.copy()` to ensure the returned arrays are safe from accidental modification by the user.