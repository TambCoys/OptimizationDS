# Algo 1 Baseline Solver Analysis

Based on the code in `algo1_baseline_solver.py` and the theoretical background from `feedback2.tex`, here is a curated, step-by-step analysis of the first custom solver. 

This analysis serves as a fantastic educational artifact because it perfectly bridges the gap between raw theory and a fully optimized implementation. It contains several naive choices that explain why it struggles to converge or scale, which perfectly motivates the need for a more advanced, optimized solver.

---

## 1. The Goal of `algo1_baseline_solver.py`
This file implements a **Feasible-Start Primal-Dual Interior-Point Method**. Unlike the general-purpose SciPy/CVXPY solvers, this algorithm is custom-built for the specific problem:
$$ \min \frac{1}{2} x^T Q x + q^T x \quad \text{s.t.} \quad Ex = 1, \ x \ge 0 $$
It attempts to solve the KKT system directly by applying Newton's method to the relaxed complementarity conditions ($xz = \mu$).

---

## 2. Step-by-Step Walkthrough & Critique

### Step 1: Initialization (The "Feasible Start")
```python
# --- Costruzione E ---
E = np.zeros((K, n))
for k, Ik in enumerate(blocks):
    E[k, Ik] = 1.0
onesK = np.ones(K)
onesn = np.ones(n)
```
*   **What it does:** Builds the dense constraint matrix $E$ where each row corresponds to a block and each column to a variable.
*   **Critique:** **Naive but correct for a baseline.** Building a dense $K \times n$ matrix is memory-intensive and ignores the sparse, block-diagonal structure of the problem. However, it makes the subsequent matrix multiplications ($E @ x$) mathematically transparent.

```python
x = np.zeros(n)
for Ik in blocks:
    x[Ik] = 1.0 / len(Ik)
```
*   **What it does:** Initializes $x$ exactly at the center of the simplices.
*   **Critique:** **Excellent.** This guarantees that $Ex = 1$ and $x > 0$ from iteration 0. This is the definition of a "primal feasible start."

```python
w = Q @ x + q

# δ = max(ε, τ · max_k range(w_Ik))
range_max = 0.0
for Ik in blocks:
    wb = w[Ik]
    if wb.size > 1:
        range_max = max(range_max, float(wb.max() - wb.min()))
delta = max(eps, tau * range_max)

y = np.zeros(K)
for k, Ik in enumerate(blocks):
    y[k] = -float(w[Ik].min()) + delta
z = w + E.T @ y
```
*   **What it does:** Initializes the dual variables $y$ (multipliers for $Ex=1$) and $z$ (multipliers for $x \ge 0$). First, it calculates the gradient of the objective at the starting point ($w = Qx + q$). Then, it calculates a shift parameter `delta` ($\delta$) based on the maximum range of $w$ within any single block. Finally, it sets $y$ such that $z$ will be strictly positive, specifically ensuring that the smallest value of $z$ in any block is exactly `delta`.
*   **Critique:** **Good, but naive.** It successfully achieves a "dual feasible start" because $Qx + q + E^Ty - z = 0$ holds exactly. The $\delta$-rule is a standard heuristic to ensure $z$ starts sufficiently far from the boundary ($z=0$). However, calculating a single global `delta` based on the maximum range across *all* blocks is a rigid heuristic. If one block has a massive range and another has a tiny range, the global `delta` might be too large for the small block, skewing the initial complementarity gap ($\mu$) and causing numerical instability immediately. A more robust approach (as seen in later optimized versions) calculates a separate $\delta_k$ for each block.

### Step 2: The Iteration Loop

#### 2.1 Residuals and Stopping Criteria
```python
rD = Q @ x + q + E.T @ y - z
rP = E @ x - onesK
# ...
xz_max = float(np.max(x * z))
if (info["rP_inf"][-1] <= eps_feas and
    info["rD_inf"][-1] <= eps_feas and
    (mu <= eps_comp or xz_max <= eps_comp)):
    break
```
*   **What it does:** Calculates the primal ($r_P$) and dual ($r_D$) residuals. It stops if both residuals are near zero and the complementarity gap ($\mu$ or the maximum $x_i z_i$) is below the tolerance.
*   **Critique:** **Standard and correct.** Because this is a feasible-start method, $r_P$ and $r_D$ should theoretically remain exactly zero throughout the algorithm. Tracking them is crucial for detecting numerical drift.

#### 2.2 The Newton System (The Core Bottleneck)
```python
H = Q + np.diag(z / x) + delta * np.eye(n)
mu_target = sigma * mu
rC = x * z - mu_target * onesn
rhs = -rD - (rC / x)
```
*   **What it does:** Constructs the Hessian matrix $H$ for the Newton step. The term `np.diag(z / x)` comes directly from linearizing the complementarity condition $xz = \mu$. The `delta * np.eye(n)` is a Tikhonov regularization term added to prevent the matrix from becoming singular. It also calculates the target complementarity $\mu_{target}$ using a fixed centering parameter $\sigma$.
*   **Critique:** **Mathematically correct, computationally disastrous.** 
    1. **Dense Matrix:** It builds $H$ as a dense $n \times n$ matrix. If $n = 10,000$, $H$ takes 800MB of RAM and is incredibly slow to invert.
    2. **Regularization Hack:** Adding `delta * np.eye(n)` alters the actual problem being solved. It changes the objective from $\frac{1}{2}x^TQx$ to $\frac{1}{2}x^T(Q + \delta I)x$. This is a major reason why the algorithm might fail to converge to the *exact* true solution—it's solving a slightly different problem to avoid math errors.
    3. **Fixed $\sigma = 0.01$:** It uses a very aggressive, fixed centering parameter. Standard IPMs use predictor-corrector methods (like Mehrotra's) to dynamically adjust $\sigma$. A fixed, aggressive $\sigma$ often causes the algorithm to crash into the boundary of the feasible region, forcing step sizes ($\alpha$) to become microscopic, stalling the algorithm.

#### 2.3 The Schur Complement (The Attempt at Optimization)
```python
H_inv_rhs = np.linalg.solve(H, rhs)
H_inv_ET  = np.linalg.solve(H, E.T)
S = E @ H_inv_ET
b = E @ H_inv_rhs + rP
```
*   **What it does:** Instead of solving a massive $(n+K) \times (n+K)$ block matrix for $\Delta x$ and $\Delta y$ simultaneously, it uses the Schur complement to reduce the problem to a smaller $K \times K$ system ($S \Delta y = b$).
*   **Critique:** **Right idea, wrong execution.** 
    *   The theory in `feedback2.tex` explicitly suggests using the Schur complement.
    *   *However*, `np.linalg.solve(H, E.T)` requires solving $K$ dense linear systems of size $n \times n$. This is $O(K \cdot n^3)$ complexity. It completely defeats the purpose of the Schur complement, which is supposed to be fast. In a truly optimized solver, you would exploit the block-diagonal structure of $E$ and the sparsity of $H$ to do this in $O(n)$ time.

#### 2.4 Directions
```python
dy = np.linalg.solve(S, b)
dx = np.linalg.solve(H, rhs - E.T @ dy)
dz = rD + Q @ dx + E.T @ dy # <-- FIX aggiungi rD
```
*   **What it does:** Solves the reduced Schur system for $\Delta y$, back-substitutes to find $\Delta x$, and finally computes $\Delta z$ using the linearized dual feasibility condition.
*   **Critique:** **Mathematically exact.** This is the correct sequence of operations to recover the full Newton step $(\Delta x, \Delta y, \Delta z)$ from the Schur complement. The explicit addition of $r_D$ in the $\Delta z$ calculation is correct, as it accounts for any numerical drift in dual feasibility.

#### 2.5 Step Size
```python
def frac_to_boundary(v, dv):
    mask = dv < 0
    if np.any(mask):
        return min(1.0, 0.99 * float(np.min(-v[mask] / dv[mask])))
    return 1.0

alpha = min(frac_to_boundary(x, dx), frac_to_boundary(z, dz))
```
*   **What it does:** Calculates how far the algorithm can step in the direction $(\Delta x, \Delta y, \Delta z)$ without violating the $x > 0$ and $z > 0$ constraints. It stops at 99% of the distance to the boundary.
*   **Critique:** **Standard and correct.** This is the textbook "fraction-to-the-boundary" rule mentioned in `feedback2.tex`.

#### 2.6 Updates and Clipping
```python
x += alpha * dx
y += alpha * dy
z += alpha * dz
x = np.maximum(x, tiny)
z = np.maximum(z, tiny)
```
*   **What it does:** Updates the variables and forces them to be at least `1e-16`.
*   **Critique:** **A dangerous band-aid.** The `np.maximum` clip destroys the strict feasibility that the algorithm worked so hard to maintain. If $x$ is clipped, $Ex = 1$ is no longer strictly true, and the "feasible-start" property is broken. This causes the residuals $r_P$ and $r_D$ to explode in later iterations, preventing convergence.

---

## 3. Why it doesn't converge (The Verdict)
This baseline solver is a perfect translation of the math equations into code, but it ignores numerical linear algebra realities. It fails to converge because:
1. **The `tiny` clipping** breaks the primal-dual feasibility, causing the Newton steps to calculate garbage directions in later iterations.
2. **The `delta` regularization** changes the problem slightly, meaning the KKT conditions it is trying to satisfy don't match the original problem.
3. **Fixed $\sigma = 0.01$:** It uses a very aggressive, fixed centering parameter. Standard IPMs use predictor-corrector methods (like Mehrotra's) to dynamically adjust $\sigma$. A fixed, aggressive $\sigma$ often causes the algorithm to crash into the boundary of the feasible region, forcing step sizes ($\alpha$) to become microscopic, stalling the algorithm.

### Should you keep it in the project?
**Yes, absolutely.** 
It is highly recommended to keep this file in your repository (perhaps in a `naive_implementations` or `history` folder). In your final report, you can use this exact analysis to say: 
> *"We first implemented the raw mathematical theory (Algo 1). We discovered that dense matrix inversions ($O(n^3)$) and numerical instability (clipping, regularization) prevented convergence. This motivated our final, optimized solver which exploits block-sparsity and uses dynamic step-sizes."*

This shows a deep understanding of the engineering process, not just the math.